<div align="center">

# Desarrollo prueba tÃ©cnica Colsubsidio

**Responsable:** Wuilson Adolfo Estacio Rojas<br/>  
**Analista de incorporaciÃ³n:** Yeimmy VerÃ³nica Bustos Moreno<br/> 
**Jefe gestiÃ³n de la informaciÃ³n:** Camilo GarzÃ³n MÃ¡rquez<br/> 
**Fecha:** 15-09-2025  

</div>


# Prueba Tecnica Cientifico de Datos - Colsubsidio 

Este documento describe el proceso seguido para **explorar** los datos, **identificar** variables relevantes y **definir** un modelo heurÃ­stico para solucionar cada uno de los puntos planteados en la prueba tecnica.


## Tabla de Contenido
1. [IntroducciÃ³n](#introducciÃ³n)  
2. [Objetivo](#objetivo)  
3. [Modelo de Fuga](#modelo-de-fuga)  
   - 3.1 [ExploraciÃ³n y EvaluaciÃ³n de Datos (EDA)](#eda)  
     - 3.1.1 [Data Quality](#data-quality)  
     - 3.1.2 [EstadÃ­sticas y descriptivos](#estadÃ­sticas-descriptivas)  
   - 3.2 [DefiniciÃ³n del Modelo](#DefiniciÃ³n-del-Modelo)
     - 3.2.1 [Flujo Datos](#Flujo-Datos)  
4. [Ejercicios de numeral Dos](#ejercicios-de-numeral-dos)  
5. [AnÃ¡lisis de punto 3](#anÃ¡lisis-de-punto-3)



## introduccion

### Â¿QuÃ© es Colsubsidio?

Es una **Caja de CompensaciÃ³n Familiar en Colombia**, entidad privada, de naturaleza social y sin Ã¡nimo de lucro, que administra recursos del subsidio familiar y presta servicios a trabajadores y sus familias en distintos sectores.  

Colsubsidio fue fundada en **1957**, por lo que lleva alrededor de **68 aÃ±os** en funcionamiento en Colombia.  

Su objetivo principal es **mejorar la calidad de vida de los trabajadores afiliados y sus familias**.  
Para ello ha dividido la prestaciÃ³n de sus servicios en **once Unidades Especializadas de Servicio (UES)**.  

Una de ellas, la **UES de CrÃ©dito**, atiende las necesidades de la poblaciÃ³n (afiliados y no afiliados) a travÃ©s de productos crediticios como: **cupo, consumo e hipotecario**, a nivel nacional.  

Pero existen otras UES, como se muestra en la siguiente imagen:  

<p align="center">
  <img src="./Imagenes/UES_Colsubsidio.png" alt="UES Disponibles" title="UES Disponibles" width="700"/>
</p>

---

## Objetivo
El objetivo de la prueba es idear una soluciÃ³n para el punto uno que es el Modelo de Fuga dado en el documento Data_Scientist_Test_2023.html el cual tiene un  peso del 75%, tambien la solucion de cualquiera de los puntos del numeral dos con un peso del 15% y finalizar con el desarrollo del numeral 3 con un peso del 10%


---

# Modelo-de-fuga

### Objetivo: En este caso, se requiere establecer la probabilidad que un cliente en una de sus lÃ­neas de negocio (CrÃ©dito) se vaya o no. Para esto se tiene un histÃ³rico de datos de los clientes que han solicitado un tipo de retiro y los que no lo han expresado.

## ğŸ“‘ Columnas Relevantes

### BASE: train

- **id** â†’ IdentificaciÃ³n Ãšnica  
- **Fecha.Expedicion** â†’ Fecha expediciÃ³n del cupo  
- **Cancelacion** â†’ Tipo CancelaciÃ³n  
- **Gestionable** â†’ Describe si es posible gestionar o no la identificaciÃ³n (*Gestionable / No Gestionable*)  
- **Retencion** â†’ Se realizÃ³ una retenciÃ³n o no *(No cuenta para test)*  
- **TIPO** â†’ Tipo de tarjeta  
- **ANO_MES** â†’ AÃ±o mes de la intenciÃ³n de cancelaciÃ³n  
- **Target** â†’ Variable Objetivo o Target *(No cuenta para test)*  
- **Fecha.Proceso** â†’ Fecha anÃ¡lisis de la base  
- **Disponible.Avances** â†’ Saldo disponible para avances de efectivo  
- **Limite.Avances** â†’ Saldo lÃ­mite para avances de efectivo  
- **Total.Intereses** â†’ Total intereses pagados mes anÃ¡lisis  
- **Saldos.Mes.Ant** â†’ Saldo del mes anterior  
- **Pagos.Mes.Ant** â†’ Valor en COP de los pagos del mes anterior  
- **Vtas.Mes.Ant** â†’ Consumo mes anterior  
- **Edad.Mora** â†’ DÃ­as en mora  
- **Limite.Cupo** â†’ Cupo lÃ­mite  
- **Pago.del.Mes** â†’ Valor en COP de los pagos del mes  
- **Pago.Minimo** â†’ Valor en COP del pago mÃ­nimo  
- **Vr.Mora** â†’ Valor en COP de la mora  
- **Vr.Cuota.Manejo** â†’ Valor de la cuota de manejo  
- **Saldo** â†’ Saldo disponible en el mes de anÃ¡lisis  

---

### BASE: test_demograficas

- **id** â†’ IdentificaciÃ³n Ãšnica  
- **categoria** â†’ CategorÃ­a de afiliado a la caja Colsubsidio  
- **segmento** â†’ Segmento poblacional *(segÃºn modelo analÃ­tico desarrollado en la caja)*  
- **edad** â†’ Edad en aÃ±os  
- **nivel_educativo** â†’ Nivel educativo *(Sin definir, primaria, secundaria, profesional, posgrado)*  
- **estado_civil** â†’ Estado civil actual  
- **Genero** â†’ GÃ©nero  
- **PAC** â†’ NÃºmero de personas a cargo  
- **contrato** â†’ Tipo de contrato *(1: Fijo, 2: Indefinido, 3: PrestaciÃ³n Servicios, 4: Independiente)*  
- **estrato** â†’ Estrato socioeconÃ³mico  

---

### BASE: test_subsidio

- **cuota_monetaria** â†’ (1: Tiene derecho a cuota monetaria, 2: No tiene derecho)  
- **sub_vivenda** â†’ (1: Ha solicitado y desembolsado subsidio de vivienda, 2: No ha solicitado)  
- **bono_lonchera** â†’ (1: Tiene derecho a Bono Lonchera, 2: No tiene derecho)  
---


## EDA
Para este modelo de fuga inicialmente para ello cargamos los datos de train (train_test_demograficas,train_test_subsidios, train) para la exploracion de los datos, con un total de 50001 clientes unicos o Id unicos, esta exploracion se realiza en el cuaderdo de python Modelo de Fuga -eda

### Data-Quality
1 Valores nulos se identificaron solo en la siguientes colunas:
- Gestionable:           48589   â†’   0.97
- Cancelacion:           48589   â†’   0.97
- ANO_MES:               48589   â†’   0.97
- TIPO:                  48589   â†’   0.97 
- Retencion:             48589   â†’   0.97
- estrato:               45885   â†’   0.97
- aqui claramente observamos el tipo de columna, su cantidad en nulos y su porcentaje respecto al total de los datos.
  
2 Duplicados
- No se encontraron Id duplicados
- No se encontraron casos donde Fecha.Proceso < Fecha.Expedicion

### EstadÃ­sticas-descriptivas
De los datos se encontraron 1412 cancelaciones, iniciando con una en 2017-01-01, de fecha maxima en 2018-03-01, donde los motivos mas frecuentes tipos de cancelacion son los siguientes
<p align="center">
  <img src="./Imagenes/top 10 Motivos Cancelacion1.png", title="top 10 Motivos Cancelacion" width="700"/>
</p>

- **TIPO (`Tipo de Tarjeta que tenian quienes se fueron`)**  
  - > Tipo de tarjeta: Cupo â†’ 1106, Amparo â†’ 306

- **Retencion (`Se realizÃ³ una retenciÃ³n o no`)**
  - > Tipo Retencion:  no efectiva  â†’ 1239, efectiva â†’  173
     
- **Genero (`Con que genero se identifica el cliente Hombre o Mujer`)**
  - > Tipo Genero:  Masculino â†’ 25126 â†’ 0.50% , Femenino â†’ 24875 â†’ 0.49%

- **Edad (`Edad de los clinetes`)**
  - MÃ¡ximo: 65
  - MÃ­nimo: 18
  - Promedio:  41 
  - DesviaciÃ³n EstÃ¡ndar: ~13

- **PAC (`NÃºmero de personas a cargo`)**
  - MÃ¡ximo: 63
  - MÃ­nimo: 0
  - Promedio:  2 
  - DesviaciÃ³n EstÃ¡ndar: ~0.1

- **Meses De Duracion antes de churn (`NÃºmero de meses que duro la persona afiliada antes de churn`)**
  - MÃ¡ximo: 138
  - MÃ­nimo: 0
  - Promedio:  27
  - Mediana:   16
  - q1:   8
  - q3:   39 

- **Dias de la semana que mas realizaron churn**
- De las bajas organizadas por dia podemos ver que el dia domingo es el dia que mayor presenta bajas
<p align="center">
  <img src="./Imagenes/Bajas por dia.png", title="Bajas por dia" width="600"/>
</p>


- **Bajas por mes (`Cantidad de clientes que se fueron de a cuerdo al mes`)**
- De las bajas organizadas por mes podemos ver que el mes de diciembre presenta mas casos
<p align="center">
  <img src="./Imagenes/Bajas por mes.png", title="Bajas por mes" width="600"/>
</p>

### AnÃ¡lisis de bajas por mes

## Lo que muestran los datos

- **Enero 2017 (157)** y **diciembre 2017 (181)** fueron los meses con mÃ¡s bajas.  
- Entre **febrero y noviembre de 2017**, las salidas se mantuvieron estables dentro del rango **80â€“116**.  
- En **2018**, los meses medidos (febrero con 71 y marzo con 77) aÃºn muestran cifras relevantes, aunque ligeramente por debajo del promedio de 2017.  
- El mÃ­nimo fue **81 en abril 2017** y el mÃ¡ximo **181 en diciembre 2017**, casi el doble de diferencia entre un mes y otro.

## Posibles patrones

- **Estacionalidad / fin de aÃ±o:**  
  Diciembre marca un pico que puede relacionarse con gastos de temporada, ajustes financieros o vencimientos de contratos/subsidios.  
  Enero tambiÃ©n aparece alto, lo que podrÃ­a ser un efecto rezagado del mismo ciclo.  

- **Estabilidad en el resto del aÃ±o:**  
  Entre febrero y noviembre de 2017, las bajas se mantuvieron bastante homogÃ©neas (alrededor de **85â€“110**).  
  Esto sugiere que, fuera de los picos de inicio y cierre de aÃ±o, la salida de clientes sigue un patrÃ³n constante.  

- **Tendencia en 2018:**  
  En los datos disponibles, los primeros meses de 2018 muestran una **leve reducciÃ³n** comparados con el promedio de 2017, aunque aÃºn dentro del rango de los meses mÃ¡s bajos del aÃ±o anterior.  
  Esto podrÃ­a indicar ajustes en polÃ­ticas o mejoras en la retenciÃ³n de clientes.

- **Nota** segun los datos y teniendo una vista general a ellos nos percatamos que la mayor cantidad es estos estan agrupados en la fecha 2018-04 con un total de 48589 lo cuales mas del 90% de los datos agrupados en una sola fecha.
  
---

- **DistribuciÃ³n de GÃ©nero y nivel_educativo (`DistribuciÃ³n de GÃ©nero y nivel_educativo`)**
<p align="center">
  <img src="./Imagenes/DistribuciÃ³n de GÃ©nero y nivel_educativo.png" alt="DistribuciÃ³n de GÃ©nero y nivel_educativo" title="DistribuciÃ³n de GÃ©nero y nivel_educativo" width="600"/>
</p>

## Hallazgos principales de acuerdo a DistribuciÃ³n de GÃ©nero y nivel_educativo

### Diferencias por gÃ©nero

**Mujeres (F):**
- Mayor proporciÃ³n en **primaria** (340 casos).  
- Le siguen **tÃ©cnico/tecnolÃ³gico** (273) y **secundaria** (71).  

**Hombres (M):**
- TambiÃ©n dominan en **primaria** (397 casos), que es incluso mayor que en mujeres.  
- Luego **tÃ©cnico/tecnolÃ³gico** (260) y **secundaria** (71).  

**PatrÃ³n comÃºn:**  
En ambos gÃ©neros se mantiene la jerarquÃ­a:  
`primaria > tÃ©cnico/tecnolÃ³gico > secundaria`

---

### ConcentraciÃ³n en nivel educativo bajo
- En ambos gÃ©neros, el grueso de las salidas ocurre en personas con **primaria**.  
- Esto sugiere que el **nivel educativo podrÃ­a estar asociado a mayor probabilidad de salida**, reflejando posible menor estabilidad o menor retenciÃ³n.  

---

### ComparaciÃ³n de gÃ©nero en el mismo nivel educativo
- En **primaria**, los hombres que se fueron (397) superan a las mujeres (340).  
- En **tÃ©cnico/tecnolÃ³gico**, las mujeres tienen una ligera ventaja (273 vs 260).  
- En **secundaria**, ambos gÃ©neros presentan exactamente la misma cantidad (71).  

---

### Implicaciones estadÃ­sticas
- Existe un **patrÃ³n homogÃ©neo de abandono segÃºn nivel educativo**: la **primaria concentra la mayor pÃ©rdida** en ambos gÃ©neros.  
- La diferencia mÃ¡s visible se da en **primaria**, con hombres mÃ¡s afectados que mujeres.  
- Los niveles educativos mÃ¡s altos (**tÃ©cnico/tecnolÃ³gico**) no eliminan la salida, pero presentan cifras menores que primaria.  


## **Diagnostic Plots (`Diagnostico de variables continuas`)**
<p align="center">
  <img src="./Imagenes/Diagnostic1.png", title="Diagnostic1" width="600"/>
</p>

<p align="center">
  <img src="./Imagenes/Diagnostic2.png", title="Diagnostic2" width="600"/>
</p>

<p align="center">
  <img src="./Imagenes/Diagnostic3.png", title="Diagnostic3" width="600"/>
</p>


## AnÃ¡lisis EstadÃ­stico de Variables

### ğŸ”¹ Edad
- DistribuciÃ³n bastante uniforme entre **20 y 65 aÃ±os**.  
- **Media y mediana â‰ˆ 40 aÃ±os** â†’ poblaciÃ³n balanceada.  
- **Skew â‰ˆ 0** y **curtosis negativa** â†’ no hay colas largas ni concentraciÃ³n fuerte.  
- Variable relativamente estable.  

---

### ğŸ”¹ Saldo y Saldos.Mes.Ant
- Claramente **asimÃ©tricos a la derecha** (Skew ~ 6, curtosis > 60).  
- La mayorÃ­a presenta saldos bajos, pero existen pocos casos con montos muy elevados.  
- En el **boxplot** se observa gran cantidad de **outliers sobre el percentil 75**.  

---

### ğŸ”¹ Pagos.Mes.Ant y Vtas.Mes.Ant
- Presentan **sesgo extremo** (Skew > 20, curtosis > 1000).  
- La gran mayorÃ­a de usuarios tiene consumos/pagos bajos.  
- Unos pocos concentran **montos millonarios**.  

---

### ğŸ”¹ Edad.Mora
- La **mediana es cero dÃ­as** â†’ la mayorÃ­a estÃ¡ al dÃ­a.  
- Existen **outliers que alcanzan miles de dÃ­as en mora**.  
- **Sesgo positivo (Skew ~ 6)**, indicador de concentraciÃ³n fuerte en pocos casos crÃ­ticos.  

---

### ğŸ”¹ Intereses y Cuotas de Manejo
- Colas largas a la derecha.  
- La mayorÃ­a paga montos bajos.  
- Existen **valores atÃ­picos muy altos** en pocos individuos.  

---

## ğŸ”¹ Conclusiones Globales
- **Edad**: variable estable y representativa.  
- **Variables financieras** (Saldo, Pagos, Ventas, Intereses, Avances) â†’ presentan **alta asimetrÃ­a y outliers**, requieren **normalizaciÃ³n/transformaciÃ³n**.  
- **Edad.Mora**: clave para segmentar riesgo â†’ mayorÃ­a sin mora vs minorÃ­a altamente morosa.  
- **AÃ‘O_MES**: evidencia **sesgo temporal** â†’ debe controlarse en el anÃ¡lisis.  

---

## DefiniciÃ³n-del-Modelo
La preparaciÃ³n y preprocesamiento de los datos se llevÃ³ a cabo en el cuaderno "Preprocesamiento Data". En esta etapa se realizÃ³ la carga de los datos de entrenamiento, la limpieza y depuraciÃ³n de las variables, asÃ­ como la transformaciÃ³n y normalizaciÃ³n de los atributos relevantes del conjunto de datos. Se analizaron posibles correlaciones temporales de la variable objetivo y se evaluÃ³ el impacto potencial de las columnas de tipo fecha sobre el target.

- Durante el proceso, se trataron valores atÃ­picos y se aplicaron tÃ©cnicas de normalizaciÃ³n utilizando el metodo de winsorize_serie, para garantizar la calidad y consistencia de los datos de entrada. AdemÃ¡s, mediante el anÃ¡lisis de series temporales y la inspecciÃ³n de matrices de correlaciÃ³n, se comprobÃ³ que no existÃ­a una correlaciÃ³n significativa entre la variable objetivo y la segmentaciÃ³n por meses. Esto permitiÃ³ descartar patrones estacionales laborales y validar la idoneidad de las fechas como predictoras en el modelo.

### Flujo-Datos

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pasos para un desarrollo efectivo  â”‚    
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Entendimiento       â”‚ 
â”‚   del problema       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DESCARGA DE DATOS   â”‚ 
â”‚  (Ingesta de Datos)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚1. RECEPCIÃ“N DE DATOS (ARCHIVOS CSV, PARQUET, ETC.) â”‚
â”‚   - Lectura de los ficheros                        â”‚
â”‚   - Otras fuentes de datos                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚2. PREPROCESAMIENTO Y VALIDACIÃ“N                       â”‚
â”‚   - Limpieza de registros (valores nulos, duplicados) â”‚
â”‚   - Formateo de fechas (datetime)                     â”‚
â”‚   - ConversiÃ³n de tipos                               â”‚
â”‚   - Garantizar la integridad de los datos             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚3. APLICACIÃ“N DE LA LÃ“GICA (REGLA DE NEGOCIOS)    â”‚
â”‚   - Para cada transacciÃ³n, verificar si cumplen  â”‚ 
â”‚     el criterio o los criterios del negocio      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚4. GENERACIÃ“N DE ATRIBUTOS (FEATURES)                           â”‚
â”‚   - Calculo de diferencia en dias, meses                       |
|   - Creacion de nuevas columnas explicativas                   |
|   - Verificacion y calidad de todos los datos                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚5. Generacion del Modelo                                                     â”‚
â”‚   - Se generan varios modelos para poder obtener el de mejores resultados   |
â”‚   - Ajuste de Hiperparametros y nueva selecion del mejor modelo             | 
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚6 . Carga de datos nuevamente a la nube                      |
|    para disponibilidad del usuario,                         â”‚
â”‚   - mediante Azure o AWS                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚5. SALIDA                                                    â”‚
â”‚   - Almacenamiento de resultados (CSV, Base de datos, etc.) â”‚
â”‚   - Consumir los resultados en dashboards                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FIN DEL PROCESO  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<p align="center">
  <img src="./Imagenes/Correlacion variable target.png", title="Correlacion variable target" width="600"/>
</p>

<p align="center">
  <img src="./Imagenes/Matriz correlacion1.png", title="Matriz correlacion1" width="600"/>
</p>

## AnÃ¡lisis de CorrelaciÃ³n

El anÃ¡lisis de correlaciÃ³n evidencia que ninguna variable presenta una relaciÃ³n lineal fuerte con el Target (churn): todas las correlaciones se mantienen dentro del rango Â±0.06.
Esto sugiere que la fuga no se explica por predictores individuales, sino por la combinaciÃ³n de mÃºltiples factores no lineales.

ğŸ”¹ Patrones observados

Saldo (â‰ˆ -0.06): correlaciÃ³n negativa â†’ clientes con mayores saldos presentan menor probabilidad de fuga.

Pagos recientes (â‰ˆ +0.04): correlaciÃ³n positiva dÃ©bil â†’ podrÃ­an reflejar pagos de cierre antes de abandonar.

Total de intereses (â‰ˆ -0.04): ligera correlaciÃ³n negativa â†’ clientes con mayor carga de intereses tienden a mantenerse activos.

Edad y estado civil: no muestran relaciÃ³n estadÃ­sticamente significativa con el churn.

ğŸ”¹ Hallazgos adicionales

Existe multicolinealidad entre variables financieras (Pagos.Mes.Ant, Pago.del.Mes, Saldo, Total.Intereses), lo que sugiere:

Eliminar variables redundantes, o

Aplicar reducciÃ³n de dimensionalidad (ej. PCA).

ğŸ”¹ ImplicaciÃ³n para el modelado

El churn en este dataset no depende de variables lineales aisladas, sino de interacciones complejas.
Por lo tanto, se recomienda:

Usar modelos no lineales y multivariados: Ã¡rboles de decisiÃ³n, ensembles (Random Forest, XGBoost, LightGBM), entre otros.

Complementar con tÃ©cnicas de selecciÃ³n de variables (mutual information, feature importance) para identificar los predictores mÃ¡s relevantes.


---

# Ejercicios-de-numeral-dos
El punto A indica lo siguiente:
A. (15%) Desarrollar una pregunta de diferencia de grupos para resolver con prueba de hipÃ³tesis (H1, H0)
y se nos pide resolver lo siguiente:
- Obtener descriptivos: media, mediana, asimetrÃ­a
- Realizar pruebas de normalidad (plantear la hipÃ³tesis de normalidad)
- Aplicar a los mismos datos, tanto la prueba paramÃ©trica como la no paramÃ©trica para dos grupos independientes (Contestar a la pregunta realizada con base en los resultados de las pruebas).

```python
# --- 0. Importar las bibliotecas necesarias ---
import numpy as np
from scipy import stats

# --- 1. Definir los datos de los grupos ---
grupo1_data = [
    66.180, 41.420, 81.880, 67.205, 58.626, 64.678, 74.439, 98.250, 39.465, 75.064,
    59.585, 66.086, 66.616, 41.374, 66.9, 39, 55.405, 46.824, 64.529, 64.517,
    65.166, 70.703, 77.391, 47.910, 66.116, 63.797, 53.051, 69.012, 60.368, 49.748,
    39.8, 34, 37.602, 66.948, 68.314, 85.354, 69.872, 85.009, 58.953, 41.744,
    91.509, 61.548, 37.981, 86.317, 59.479, 57.588, 53.0, 59, 46.234, 87.828,
    66.038, 65.175, 60.214, 74.662
]

grupo2_data = [
    0.621, 0.867, 0.550, 0.658, 0.794, 0.738, 0.855, 0.708, 0.774, 0.700,
    0.776, 0.904, 0.751, 0.921, 0.724, 0.754, 0.568, 0.867, 0.601, 0.725,
    0.798, 0.776, 0.835, 0.816, 0.842, 0.824, 0.706, 0.802, 0.738, 0.975,
    0.859, 0.644, 0.638, 0.809, 0.658, 0.824, 0.603, 0.855, 0.728, 0.838,
    0.932, 0.782, 0.727, 0.829, 0.809, 0.907, 0.871, 0.686, 0.750, 0.745, 0.662
]

# Convertir las listas a arrays de NumPy para facilitar los cÃ¡lculos
grupo1 = np.array(grupo1_data)
grupo2 = np.array(grupo2_data)

# Nivel de significancia
alpha = 0.05

print("="*50)
print("ANÃLISIS ESTADÃSTICO PARA DOS GRUPOS INDEPENDIENTES")
print("="*50)

# --- 2. Obtener descriptivos: media, mediana, asimetrÃ­a ---
print("\n--- A. ANÃLISIS DESCRIPTIVO ---")
print(f"Grupo 1 (n={len(grupo1)}):")
print(f"  Media: {np.mean(grupo1):.3f}")
print(f"  Mediana: {np.median(grupo1):.3f}")
print(f"  AsimetrÃ­a (Skewness): {stats.skew(grupo1):.3f}\n")

print(f"Grupo 2 (n={len(grupo2)}):")
print(f"  Media: {np.mean(grupo2):.3f}")
print(f"  Mediana: {np.median(grupo2):.3f}")
print(f"  AsimetrÃ­a (Skewness): {stats.skew(grupo2):.3f}")

# --- 3. Realizar pruebas de normalidad (Shapiro-Wilk) ---
print("\n--- B. PRUEBAS DE NORMALIDAD (Shapiro-Wilk) ---")
print("Hâ‚€: Los datos siguen una distribuciÃ³n normal.")
print("Hâ‚: Los datos no siguen una distribuciÃ³n normal.")

# Grupo 1
stat_s1, p_s1 = stats.shapiro(grupo1)
print(f"\nGrupo 1: EstadÃ­stico W = {stat_s1:.3f}, p-valor = {p_s1:.3f}")
if p_s1 > alpha:
    print(f"  DecisiÃ³n: No se rechaza Hâ‚€. Se asume normalidad (p > {alpha}).")
else:
    print(f"  DecisiÃ³n: Se rechaza Hâ‚€. No se asume normalidad (p <= {alpha}).")

# Grupo 2
stat_s2, p_s2 = stats.shapiro(grupo2)
print(f"\nGrupo 2: EstadÃ­stico W = {stat_s2:.3f}, p-valor = {p_s2:.3f}")
if p_s2 > alpha:
    print(f"  DecisiÃ³n: No se rechaza Hâ‚€. Se asume normalidad (p > {alpha}).")
else:
    print(f"  DecisiÃ³n: Se rechaza Hâ‚€. No se asume normalidad (p <= {alpha}).")

# --- 4. Aplicar pruebas paramÃ©trica y no paramÃ©trica ---

# Primero, comprobamos la homogeneidad de varianzas para la prueba t
print("\n--- C. PRUEBA DE HOMOGENEIDAD DE VARIANZAS (Levene) ---")
print("Hâ‚€: Las varianzas de los grupos son iguales.")
print("Hâ‚: Las varianzas de los grupos son diferentes.")
stat_l, p_l = stats.levene(grupo1, grupo2)
print(f"\nResultado: EstadÃ­stico F = {stat_l:.2f}, p-valor = {p_l}")
if p_l > alpha:
    print(f"  DecisiÃ³n: No se rechaza Hâ‚€. Se asume que las varianzas son iguales (p > {alpha}).")
    equal_variances = True
else:
    print(f"  DecisiÃ³n: Se rechaza Hâ‚€. Las varianzas son diferentes (p <= {alpha}).")
    equal_variances = False

print("\n--- D. APLICACIÃ“N DE PRUEBAS DE HIPÃ“TESIS ---")
print("Pregunta: Â¿Existe una diferencia significativa entre los dos grupos?")
print("Hâ‚€: Î¼â‚ = Î¼â‚‚ (No hay diferencia entre las medias de los grupos)")
print("Hâ‚: Î¼â‚ â‰  Î¼â‚‚ (Existe una diferencia entre las medias de los grupos)")

# Prueba ParamÃ©trica: t de Student para muestras independientes
print("\n1. Prueba ParamÃ©trica: t de Student para Muestras Independientes")
if not equal_variances:
    print("(Se aplicarÃ¡ la correcciÃ³n de Welch porque las varianzas no son iguales)")

stat_t, p_t = stats.ttest_ind(grupo1, grupo2, equal_var=equal_variances)
print(f"  EstadÃ­stico t = {stat_t:.3f}")
print(f"  p-valor = {p_t}")
if p_t < alpha:
    print(f"  ConclusiÃ³n: Se rechaza Hâ‚€. Existe una diferencia estadÃ­sticamente significativa entre los grupos (p < {alpha}).")
else:
    print(f"  ConclusiÃ³n: No se rechaza Hâ‚€. No hay evidencia de una diferencia significativa (p >= {alpha}).")


# Prueba No ParamÃ©trica: U de Mann-Whitney
print("\n2. Prueba No ParamÃ©trica: U de Mann-Whitney")
stat_u, p_u = stats.mannwhitneyu(grupo1, grupo2, alternative='two-sided')
print(f"  EstadÃ­stico U = {stat_u}")
print(f"  p-valor = {p_u}")
if p_u < alpha:
    print(f"  ConclusiÃ³n: Se rechaza Hâ‚€. Existe una diferencia estadÃ­sticamente significativa entre las distribuciones de los grupos (p < {alpha}).")
else:
    print(f"  ConclusiÃ³n: No se rechaza Hâ‚€. No hay evidencia de una diferencia significativa (p >= {alpha}).")

print("\n" + "="*50)
print("CONCLUSIÃ“N FINAL DEL EJERCICIO")
print("="*50)
print("Ambas pruebas, la paramÃ©trica (t de Welch) y la no paramÃ©trica (U de Mann-Whitney),")
print(f"arrojaron p-valores extremadamente bajos (p < {alpha}), lo que proporciona evidencia")
print("contundente para rechazar la hipÃ³tesis nula. Por lo tanto, se concluye que")
print("existe una diferencia muy significativa entre el Grupo 1 y el Grupo 2.")
```
Con lo que tendremos el siguiente analisis estadistico

==================================================
### ANÃLISIS ESTADÃSTICO PARA DOS GRUPOS INDEPENDIENTES
==================================================

--- A. ANÃLISIS DESCRIPTIVO ---
Grupo 1 (n=54):
  - Media: 62.138
  - Mediana: 64.523
  - AsimetrÃ­a (Skewness): 0.116

Grupo 2 (n=51):
  - Media: 0.767
  - Mediana: 0.776
  - AsimetrÃ­a (Skewness): -0.215

--- B. PRUEBAS DE NORMALIDAD (Shapiro-Wilk) --- <br/>
Hâ‚€: Los datos siguen una distribuciÃ³n normal.<br/>
Hâ‚: Los datos no siguen una distribuciÃ³n normal.

Grupo 1: EstadÃ­stico W = 0.969, p-valor = 0.171
  - DecisiÃ³n: No se rechaza Hâ‚€. Se asume normalidad (p > 0.05).

Grupo 2: EstadÃ­stico W = 0.987, p-valor = 0.860
  - DecisiÃ³n: No se rechaza Hâ‚€. Se asume normalidad (p > 0.05).
...
Ambas pruebas, la paramÃ©trica (t de Welch) y la no paramÃ©trica (U de Mann-Whitney),
arrojaron p-valores extremadamente bajos (p < 0.05), lo que proporciona evidencia
contundente para rechazar la hipÃ³tesis nula. Por lo tanto, se concluye que
existe una diferencia muy significativa entre el Grupo 1 y el Grupo 2. consistente bajo mÃ©todos paramÃ©tricos y no paramÃ©tricos.

---
El punto D indica lo siguiente:
D. (15%) Con los siguientes datos, realizar un ANOVA factorial:

- Realizar pruebas de normalidad (plantear la hipÃ³tesis de normalidad).
- (Contestar a la pregunta realizada con base en los resultados de laspruebas).


```R
# Datos originales
medida <- c(2.1, 2.2, 1.8, 2, 1.9, 2.2, 2.6, 2.7, 2.5, 2.8, 
            1.8, 1.9, 1.6, 2, 1.9, 2.1, 2, 2.2, 2.4, 2.1)

factorA <- gl(4, 5)
factorB <- factor(rep(1:5, 4))

# Crear data frame
datos <- data.frame(medida, factorA, factorB)

# Modelo ANOVA sin interacciÃ³n (efectos principales solamente)
modelo <- aov(medida ~ factorA + factorB, data = datos)

# Verificar residuos
residuos <- resid(modelo)
print("Primeros residuos:")
print(head(residuos))

# Prueba de normalidad (ahora funcionarÃ¡)
shapiro_resultado <- shapiro.test(residuos)
print(shapiro_resultado)

# ANOVA factorial
print("ANOVA - Efectos principales:")
summary(modelo)

```
### Resultados

<p align="center">
  <img src="./Imagenes/Punto 2 D en R.png", title="Solucion Punto 2-D" width="600"/>
</p>

==============================
### ANÃLISIS ESTADÃSTICO 
==============================
Para interpretar un ANOVA, es necesario comprobar que los residuos del modelo siguen una distribuciÃ³n normal.

- HipÃ³tesis nula (Hâ‚€): los residuos provienen de una distribuciÃ³n normal.

- HipÃ³tesis alternativa (Hâ‚): los residuos no provienen de una distribuciÃ³n normal.

Dado que el test de Shapiro-Wilk aplicado a los residuos del modelo arrojÃ³ un estadÃ­stico W = 0.943 y un p-valor = 0.275.
Dado que p > 0.05, no se rechaza la hipÃ³tesis nula, lo cual indica que el supuesto de normalidad se cumple.

por lo que:
- El factor A presenta un efecto altamente significativo sobre la variable medida (p < 0.001), osea influye significativamente en la variable respuesta.

- El factor B no muestra un efecto estadÃ­sticamente significativo (p â‰ˆ 0.64), sobre la variable respuesta.



---

# AnÃ¡lisis-de-punto-3
