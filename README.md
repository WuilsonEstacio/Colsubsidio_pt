<div align="center">

# Desarrollo prueba t√©cnica Colsubsidio

**Responsable:** Wuilson Adolfo Estacio Rojas<br/>  
**Analista de incorporaci√≥n:** Yeimmy Ver√≥nica Bustos Moreno<br/> 
**Jefe gesti√≥n de la informaci√≥n:** Camilo Garz√≥n M√°rquez<br/> 
**Fecha:** 15-09-2025  

</div>


# Prueba Tecnica Cientifico de Datos - Colsubsidio 

Este documento describe el proceso seguido para **explorar** los datos, **identificar** variables relevantes y **definir** un modelo heur√≠stico para solucionar cada uno de los puntos planteados en la prueba tecnica.


## Tabla de Contenido
1. [Introducci√≥n](#introducci√≥n)  
2. [Objetivo](#objetivo)  
3. [Modelo de Fuga](#modelo-de-fuga)  
   - 3.1 [Exploraci√≥n y Evaluaci√≥n de Datos (EDA)](#eda)  
     - 3.1.1 [Data Quality](#data-quality)  
     - 3.1.2 [Estad√≠sticas y descriptivos](#estad√≠sticas-descriptivas)  
   - 3.2 [Definici√≥n del Modelo](#Definici√≥n-del-Modelo)
     - 3.2.1 [Flujo Datos](#Flujo-Datos)
     - 3.2.3 [Entrenamiento del modelo](#Entrenamiento_del_Modelo_de_Fuga)
4. [Ejercicios de numeral Dos](#ejercicios-de-numeral-dos)  
5. [An√°lisis de punto 3](#an√°lisis-de-punto-3)
 - 5.1.1 [Interpretaci√≥n t√©cnica de la matriz](#Interpretaci√≥n_t√©cnica_de_la_matriz)
 - 5.1.2 [Cu√°les y cu√°ntas m√©tricas puede obtener de dicha matriz](#Metricas_para_dicha_Matriz) 



## introduccion

### ¬øQu√© es Colsubsidio?

Es una **Caja de Compensaci√≥n Familiar en Colombia**, entidad privada, de naturaleza social y sin √°nimo de lucro, que administra recursos del subsidio familiar y presta servicios a trabajadores y sus familias en distintos sectores.  

Colsubsidio fue fundada en **1957**, por lo que lleva alrededor de **68 a√±os** en funcionamiento en Colombia.  

Su objetivo principal es **mejorar la calidad de vida de los trabajadores afiliados y sus familias**.  
Para ello ha dividido la prestaci√≥n de sus servicios en **once Unidades Especializadas de Servicio (UES)**.  

Una de ellas, la **UES de Cr√©dito**, atiende las necesidades de la poblaci√≥n (afiliados y no afiliados) a trav√©s de productos crediticios como: **cupo, consumo e hipotecario**, a nivel nacional.  

Pero existen otras UES, como se muestra en la siguiente imagen:  

<p align="center">
  <img src="./Imagenes/UES_Colsubsidio.png" alt="UES Disponibles" title="UES Disponibles" width="700"/>
</p>

---

## Objetivo
El objetivo de la prueba es idear una soluci√≥n para el punto uno que es el Modelo de Fuga dado en el documento Data_Scientist_Test_2023.html el cual tiene un  peso del 75%, tambien la solucion de cualquiera de los puntos del numeral dos con un peso del 15% y finalizar con el desarrollo del numeral 3 con un peso del 10%


---

# Modelo-de-fuga

### Objetivo: En este caso, se requiere establecer la probabilidad que un cliente en una de sus l√≠neas de negocio (Cr√©dito) se vaya o no. Para esto se tiene un hist√≥rico de datos de los clientes que han solicitado un tipo de retiro y los que no lo han expresado.

## üìë Columnas Relevantes

### BASE: train

- **id** ‚Üí Identificaci√≥n √önica  
- **Fecha.Expedicion** ‚Üí Fecha expedici√≥n del cupo  
- **Cancelacion** ‚Üí Tipo Cancelaci√≥n  
- **Gestionable** ‚Üí Describe si es posible gestionar o no la identificaci√≥n (*Gestionable / No Gestionable*)  
- **Retencion** ‚Üí Se realiz√≥ una retenci√≥n o no *(No cuenta para test)*  
- **TIPO** ‚Üí Tipo de tarjeta  
- **ANO_MES** ‚Üí A√±o mes de la intenci√≥n de cancelaci√≥n  
- **Target** ‚Üí Variable Objetivo o Target *(No cuenta para test)*  
- **Fecha.Proceso** ‚Üí Fecha an√°lisis de la base  
- **Disponible.Avances** ‚Üí Saldo disponible para avances de efectivo  
- **Limite.Avances** ‚Üí Saldo l√≠mite para avances de efectivo  
- **Total.Intereses** ‚Üí Total intereses pagados mes an√°lisis  
- **Saldos.Mes.Ant** ‚Üí Saldo del mes anterior  
- **Pagos.Mes.Ant** ‚Üí Valor en COP de los pagos del mes anterior  
- **Vtas.Mes.Ant** ‚Üí Consumo mes anterior  
- **Edad.Mora** ‚Üí D√≠as en mora  
- **Limite.Cupo** ‚Üí Cupo l√≠mite  
- **Pago.del.Mes** ‚Üí Valor en COP de los pagos del mes  
- **Pago.Minimo** ‚Üí Valor en COP del pago m√≠nimo  
- **Vr.Mora** ‚Üí Valor en COP de la mora  
- **Vr.Cuota.Manejo** ‚Üí Valor de la cuota de manejo  
- **Saldo** ‚Üí Saldo disponible en el mes de an√°lisis  

---

### BASE: test_demograficas

- **id** ‚Üí Identificaci√≥n √önica  
- **categoria** ‚Üí Categor√≠a de afiliado a la caja Colsubsidio  
- **segmento** ‚Üí Segmento poblacional *(seg√∫n modelo anal√≠tico desarrollado en la caja)*  
- **edad** ‚Üí Edad en a√±os  
- **nivel_educativo** ‚Üí Nivel educativo *(Sin definir, primaria, secundaria, profesional, posgrado)*  
- **estado_civil** ‚Üí Estado civil actual  
- **Genero** ‚Üí G√©nero  
- **PAC** ‚Üí N√∫mero de personas a cargo  
- **contrato** ‚Üí Tipo de contrato *(1: Fijo, 2: Indefinido, 3: Prestaci√≥n Servicios, 4: Independiente)*  
- **estrato** ‚Üí Estrato socioecon√≥mico  

---

### BASE: test_subsidio

- **cuota_monetaria** ‚Üí (1: Tiene derecho a cuota monetaria, 2: No tiene derecho)  
- **sub_vivenda** ‚Üí (1: Ha solicitado y desembolsado subsidio de vivienda, 2: No ha solicitado)  
- **bono_lonchera** ‚Üí (1: Tiene derecho a Bono Lonchera, 2: No tiene derecho)  
---


## EDA
Para este modelo de fuga inicialmente para ello cargamos los datos de train (train_test_demograficas,train_test_subsidios, train) para la exploracion de los datos, con un total de 50001 clientes unicos o Id unicos, esta exploracion se realiza en el cuaderdo de python Modelo de Fuga -eda

### Data-Quality
1 Valores nulos se identificaron solo en la siguientes colunas:
- Gestionable:           48589   ‚Üí   0.97
- Cancelacion:           48589   ‚Üí   0.97
- ANO_MES:               48589   ‚Üí   0.97
- TIPO:                  48589   ‚Üí   0.97 
- Retencion:             48589   ‚Üí   0.97
- estrato:               45885   ‚Üí   0.97
- aqui claramente observamos el tipo de columna, su cantidad en nulos y su porcentaje respecto al total de los datos.
  
2 Duplicados
- No se encontraron Id duplicados
- No se encontraron casos donde Fecha.Proceso < Fecha.Expedicion

### Estad√≠sticas-descriptivas
De los datos se encontraron 1412 cancelaciones, iniciando con una en 2017-01-01, de fecha maxima en 2018-03-01, donde los motivos mas frecuentes tipos de cancelacion son los siguientes
<p align="center">
  <img src="./Imagenes/top 10 Motivos Cancelacion1.png", title="top 10 Motivos Cancelacion" width="700"/>
</p>

- **TIPO (`Tipo de Tarjeta que tenian quienes se fueron`)**  
  - > Tipo de tarjeta: Cupo ‚Üí 1106, Amparo ‚Üí 306

- **Retencion (`Se realiz√≥ una retenci√≥n o no`)**
  - > Tipo Retencion:  no efectiva  ‚Üí 1239, efectiva ‚Üí  173
     
- **Genero (`Con que genero se identifica el cliente Hombre o Mujer`)**
  - > Tipo Genero:  Masculino ‚Üí 25126 ‚Üí 0.50% , Femenino ‚Üí 24875 ‚Üí 0.49%

- **Edad (`Edad de los clinetes`)**
  - M√°ximo: 65
  - M√≠nimo: 18
  - Promedio:  41 
  - Desviaci√≥n Est√°ndar: ~13

- **PAC (`N√∫mero de personas a cargo`)**
  - M√°ximo: 63
  - M√≠nimo: 0
  - Promedio:  2 
  - Desviaci√≥n Est√°ndar: ~0.1

- **Meses De Duracion antes de churn (`N√∫mero de meses que duro la persona afiliada antes de churn`)**
  - M√°ximo: 138
  - M√≠nimo: 0
  - Promedio:  27
  - Mediana:   16
  - q1:   8
  - q3:   39 

- **Dias de la semana que mas realizaron churn**
- De las bajas organizadas por dia podemos ver que el dia domingo es el dia que mayor presenta bajas
<p align="center">
  <img src="./Imagenes/Bajas por dia.png", title="Bajas por dia" width="600"/>
</p>


- **Bajas por mes (`Cantidad de clientes que se fueron de a cuerdo al mes`)**
- De las bajas organizadas por mes podemos ver que el mes de diciembre presenta mas casos
<p align="center">
  <img src="./Imagenes/Bajas por mes.png", title="Bajas por mes" width="600"/>
</p>

### An√°lisis de bajas por mes

## Lo que muestran los datos

- **Enero 2017 (157)** y **diciembre 2017 (181)** fueron los meses con m√°s bajas.  
- Entre **febrero y noviembre de 2017**, las salidas se mantuvieron estables dentro del rango **80‚Äì116**.  
- En **2018**, los meses medidos (febrero con 71 y marzo con 77) a√∫n muestran cifras relevantes, aunque ligeramente por debajo del promedio de 2017.  
- El m√≠nimo fue **81 en abril 2017** y el m√°ximo **181 en diciembre 2017**, casi el doble de diferencia entre un mes y otro.

## Posibles patrones

- **Estacionalidad / fin de a√±o:**  
  Diciembre marca un pico que puede relacionarse con gastos de temporada, ajustes financieros o vencimientos de contratos/subsidios.  
  Enero tambi√©n aparece alto, lo que podr√≠a ser un efecto rezagado del mismo ciclo.  

- **Estabilidad en el resto del a√±o:**  
  Entre febrero y noviembre de 2017, las bajas se mantuvieron bastante homog√©neas (alrededor de **85‚Äì110**).  
  Esto sugiere que, fuera de los picos de inicio y cierre de a√±o, la salida de clientes sigue un patr√≥n constante.  

- **Tendencia en 2018:**  
  En los datos disponibles, los primeros meses de 2018 muestran una **leve reducci√≥n** comparados con el promedio de 2017, aunque a√∫n dentro del rango de los meses m√°s bajos del a√±o anterior.  
  Esto podr√≠a indicar ajustes en pol√≠ticas o mejoras en la retenci√≥n de clientes.

- **Nota** segun los datos y teniendo una vista general a ellos nos percatamos que la mayor cantidad es estos estan agrupados en la fecha 2018-04 con un total de 48589 lo cuales mas del 90% de los datos agrupados en una sola fecha.
  
---

- **Distribuci√≥n de G√©nero y nivel_educativo (`Distribuci√≥n de G√©nero y nivel_educativo`)**
<p align="center">
  <img src="./Imagenes/Distribuci√≥n de G√©nero y nivel_educativo.png" alt="Distribuci√≥n de G√©nero y nivel_educativo" title="Distribuci√≥n de G√©nero y nivel_educativo" width="600"/>
</p>

## Hallazgos principales de acuerdo a Distribuci√≥n de G√©nero y nivel_educativo

### Diferencias por g√©nero

**Mujeres (F):**
- Mayor proporci√≥n en **primaria** (340 casos).  
- Le siguen **t√©cnico/tecnol√≥gico** (273) y **secundaria** (71).  

**Hombres (M):**
- Tambi√©n dominan en **primaria** (397 casos), que es incluso mayor que en mujeres.  
- Luego **t√©cnico/tecnol√≥gico** (260) y **secundaria** (71).  

**Patr√≥n com√∫n:**  
En ambos g√©neros se mantiene la jerarqu√≠a:  
`primaria > t√©cnico/tecnol√≥gico > secundaria`

---

### Concentraci√≥n en nivel educativo bajo
- En ambos g√©neros, el grueso de las salidas ocurre en personas con **primaria**.  
- Esto sugiere que el **nivel educativo podr√≠a estar asociado a mayor probabilidad de salida**, reflejando posible menor estabilidad o menor retenci√≥n.  

---

### Comparaci√≥n de g√©nero en el mismo nivel educativo
- En **primaria**, los hombres que se fueron (397) superan a las mujeres (340).  
- En **t√©cnico/tecnol√≥gico**, las mujeres tienen una ligera ventaja (273 vs 260).  
- En **secundaria**, ambos g√©neros presentan exactamente la misma cantidad (71).  

---

### Implicaciones estad√≠sticas
- Existe un **patr√≥n homog√©neo de abandono seg√∫n nivel educativo**: la **primaria concentra la mayor p√©rdida** en ambos g√©neros.  
- La diferencia m√°s visible se da en **primaria**, con hombres m√°s afectados que mujeres.  
- Los niveles educativos m√°s altos (**t√©cnico/tecnol√≥gico**) no eliminan la salida, pero presentan cifras menores que primaria.  


## **Diagnostic Plots (`Diagnostico de variables continuas`)**
<p align="center">
  <img src="./Imagenes/Diagnostic1.png", title="Diagnostic1" width="600"/>
</p>

<p align="center">
  <img src="./Imagenes/Diagnostic2.png", title="Diagnostic2" width="600"/>
</p>

<p align="center">
  <img src="./Imagenes/Diagnostic3.png", title="Diagnostic3" width="600"/>
</p>


## An√°lisis Estad√≠stico de Variables

### üîπ Edad
- Distribuci√≥n bastante uniforme entre **20 y 65 a√±os**.  
- **Media y mediana ‚âà 40 a√±os** ‚Üí poblaci√≥n balanceada.  
- **Skew ‚âà 0** y **curtosis negativa** ‚Üí no hay colas largas ni concentraci√≥n fuerte.  
- Variable relativamente estable.  

---

### üîπ Saldo y Saldos.Mes.Ant
- Claramente **asim√©tricos a la derecha** (Skew ~ 6, curtosis > 60).  
- La mayor√≠a presenta saldos bajos, pero existen pocos casos con montos muy elevados.  
- En el **boxplot** se observa gran cantidad de **outliers sobre el percentil 75**.  

---

### üîπ Pagos.Mes.Ant y Vtas.Mes.Ant
- Presentan **sesgo extremo** (Skew > 20, curtosis > 1000).  
- La gran mayor√≠a de usuarios tiene consumos/pagos bajos.  
- Unos pocos concentran **montos millonarios**.  

---

### üîπ Edad.Mora
- La **mediana es cero d√≠as** ‚Üí la mayor√≠a est√° al d√≠a.  
- Existen **outliers que alcanzan miles de d√≠as en mora**.  
- **Sesgo positivo (Skew ~ 6)**, indicador de concentraci√≥n fuerte en pocos casos cr√≠ticos.  

---

### üîπ Intereses y Cuotas de Manejo
- Colas largas a la derecha.  
- La mayor√≠a paga montos bajos.  
- Existen **valores at√≠picos muy altos** en pocos individuos.  

---

## üîπ Conclusiones Globales
- **Edad**: variable estable y representativa.  
- **Variables financieras** (Saldo, Pagos, Ventas, Intereses, Avances) ‚Üí presentan **alta asimetr√≠a y outliers**, requieren **normalizaci√≥n/transformaci√≥n**.  
- **Edad.Mora**: clave para segmentar riesgo ‚Üí mayor√≠a sin mora vs minor√≠a altamente morosa.  
- **A√ëO_MES**: evidencia **sesgo temporal** ‚Üí debe controlarse en el an√°lisis.  

---

## Definici√≥n-del-Modelo
La preparaci√≥n y preprocesamiento de los datos se llev√≥ a cabo en el cuaderno "Preprocesamiento Data". En esta etapa se realiz√≥ la carga de los datos de entrenamiento, la limpieza y depuraci√≥n de las variables, as√≠ como la transformaci√≥n y normalizaci√≥n de los atributos relevantes del conjunto de datos. Se analizaron posibles correlaciones temporales de la variable objetivo y se evalu√≥ el impacto potencial de las columnas de tipo fecha sobre el target.

- Durante el proceso, se trataron valores at√≠picos y se aplicaron t√©cnicas de normalizaci√≥n utilizando el metodo de winsorize_serie, para garantizar la calidad y consistencia de los datos de entrada. Adem√°s, mediante el an√°lisis de series temporales y la inspecci√≥n de matrices de correlaci√≥n, se comprob√≥ que no exist√≠a una correlaci√≥n significativa entre la variable objetivo y la segmentaci√≥n por meses. Esto permiti√≥ descartar patrones estacionales laborales y validar la idoneidad de las fechas como predictoras en el modelo.

### Flujo-Datos

```plaintext
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Pasos para un desarrollo efectivo  ‚îÇ    
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Entendimiento       ‚îÇ 
‚îÇ   del problema       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DESCARGA DE DATOS   ‚îÇ 
‚îÇ  (Ingesta de Datos)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ1. RECEPCI√ìN DE DATOS (ARCHIVOS CSV, PARQUET, ETC.) ‚îÇ
‚îÇ   - Lectura de los ficheros                        ‚îÇ
‚îÇ   - Otras fuentes de datos                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ2. PREPROCESAMIENTO Y VALIDACI√ìN                       ‚îÇ
‚îÇ   - Limpieza de registros (valores nulos, duplicados) ‚îÇ
‚îÇ   - Formateo de fechas (datetime)                     ‚îÇ
‚îÇ   - Conversi√≥n de tipos                               ‚îÇ
‚îÇ   - Garantizar la integridad de los datos             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ3. APLICACI√ìN DE LA L√ìGICA (REGLA DE NEGOCIOS)    ‚îÇ
‚îÇ   - Para cada transacci√≥n, verificar si cumplen  ‚îÇ 
‚îÇ     el criterio o los criterios del negocio      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ4. GENERACI√ìN DE ATRIBUTOS (FEATURES)                           ‚îÇ
‚îÇ   - Calculo de diferencia en dias, meses                       |
|   - Creacion de nuevas columnas explicativas                   |
|   - Verificacion y calidad de todos los datos                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ5. Generacion del Modelo                                                     ‚îÇ
‚îÇ   - Se generan varios modelos para poder obtener el de mejores resultados   |
‚îÇ   - Ajuste de Hiperparametros y nueva selecion del mejor modelo             | 
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ6 . Carga de datos nuevamente a la nube                      |
|    para disponibilidad del usuario,                         ‚îÇ
‚îÇ   - mediante Azure o AWS                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ5. SALIDA                                                    ‚îÇ
‚îÇ   - Almacenamiento de resultados (CSV, Base de datos, etc.) ‚îÇ
‚îÇ   - Consumir los resultados en dashboards                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FIN DEL PROCESO  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

<p align="center">
  <img src="./Imagenes/Correlacion variable target.png", title="Correlacion variable target" width="600"/>
</p>

<p align="center">
  <img src="./Imagenes/Matriz correlacion1.png", title="Matriz correlacion1" width="600"/>
</p>

## An√°lisis de Correlaci√≥n

El an√°lisis de correlaci√≥n evidencia que ninguna variable presenta una relaci√≥n lineal fuerte con el Target (churn): todas las correlaciones se mantienen dentro del rango ¬±0.06.
Esto sugiere que la fuga no se explica por predictores individuales, sino por la combinaci√≥n de m√∫ltiples factores no lineales.

üîπ Patrones observados

Saldo (‚âà -0.06): correlaci√≥n negativa ‚Üí clientes con mayores saldos presentan menor probabilidad de fuga.

Pagos recientes (‚âà +0.04): correlaci√≥n positiva d√©bil ‚Üí podr√≠an reflejar pagos de cierre antes de abandonar.

Total de intereses (‚âà -0.04): ligera correlaci√≥n negativa ‚Üí clientes con mayor carga de intereses tienden a mantenerse activos.

Edad y estado civil: no muestran relaci√≥n estad√≠sticamente significativa con el churn.

üîπ Hallazgos adicionales

Existe multicolinealidad entre variables financieras (Pagos.Mes.Ant, Pago.del.Mes, Saldo, Total.Intereses), lo que sugiere:

Eliminar variables redundantes, o

Aplicar reducci√≥n de dimensionalidad (ej. PCA).

üîπ Implicaci√≥n para el modelado

El churn en este dataset no depende de variables lineales aisladas, sino de interacciones complejas.
Por lo tanto, se recomienda:

Usar modelos no lineales y multivariados: √°rboles de decisi√≥n, ensembles (Random Forest, XGBoost, LightGBM), entre otros.

Complementar con t√©cnicas de selecci√≥n de variables (mutual information, feature importance) para identificar los predictores m√°s relevantes.
---

### Entrenamiento_del_Modelo_de_Fuga

---

# Ejercicios-de-numeral-dos
El punto A indica lo siguiente:
A. (15%) Desarrollar una pregunta de diferencia de grupos para resolver con prueba de hip√≥tesis (H1, H0)
y se nos pide resolver lo siguiente:
- Obtener descriptivos: media, mediana, asimetr√≠a
- Realizar pruebas de normalidad (plantear la hip√≥tesis de normalidad)
- Aplicar a los mismos datos, tanto la prueba param√©trica como la no param√©trica para dos grupos independientes (Contestar a la pregunta realizada con base en los resultados de las pruebas).

Ejecucion en python
```python
# --- 0. Importar las bibliotecas necesarias ---
import numpy as np
from scipy import stats

# --- 1. Definir los datos de los grupos ---
grupo1_data = [
    66.180, 41.420, 81.880, 67.205, 58.626, 64.678, 74.439, 98.250, 39.465, 75.064,
    59.585, 66.086, 66.616, 41.374, 66.9, 39, 55.405, 46.824, 64.529, 64.517,
    65.166, 70.703, 77.391, 47.910, 66.116, 63.797, 53.051, 69.012, 60.368, 49.748,
    39.8, 34, 37.602, 66.948, 68.314, 85.354, 69.872, 85.009, 58.953, 41.744,
    91.509, 61.548, 37.981, 86.317, 59.479, 57.588, 53.0, 59, 46.234, 87.828,
    66.038, 65.175, 60.214, 74.662
]

grupo2_data = [
    0.621, 0.867, 0.550, 0.658, 0.794, 0.738, 0.855, 0.708, 0.774, 0.700,
    0.776, 0.904, 0.751, 0.921, 0.724, 0.754, 0.568, 0.867, 0.601, 0.725,
    0.798, 0.776, 0.835, 0.816, 0.842, 0.824, 0.706, 0.802, 0.738, 0.975,
    0.859, 0.644, 0.638, 0.809, 0.658, 0.824, 0.603, 0.855, 0.728, 0.838,
    0.932, 0.782, 0.727, 0.829, 0.809, 0.907, 0.871, 0.686, 0.750, 0.745, 0.662
]

# Convertir las listas a arrays de NumPy para facilitar los c√°lculos
grupo1 = np.array(grupo1_data)
grupo2 = np.array(grupo2_data)

# Nivel de significancia
alpha = 0.05

print("="*50)
print("AN√ÅLISIS ESTAD√çSTICO PARA DOS GRUPOS INDEPENDIENTES")
print("="*50)

# --- 2. Obtener descriptivos: media, mediana, asimetr√≠a ---
print("\n--- A. AN√ÅLISIS DESCRIPTIVO ---")
print(f"Grupo 1 (n={len(grupo1)}):")
print(f"  Media: {np.mean(grupo1):.3f}")
print(f"  Mediana: {np.median(grupo1):.3f}")
print(f"  Asimetr√≠a (Skewness): {stats.skew(grupo1):.3f}\n")

print(f"Grupo 2 (n={len(grupo2)}):")
print(f"  Media: {np.mean(grupo2):.3f}")
print(f"  Mediana: {np.median(grupo2):.3f}")
print(f"  Asimetr√≠a (Skewness): {stats.skew(grupo2):.3f}")

# --- 3. Realizar pruebas de normalidad (Shapiro-Wilk) ---
print("\n--- B. PRUEBAS DE NORMALIDAD (Shapiro-Wilk) ---")
print("H‚ÇÄ: Los datos siguen una distribuci√≥n normal.")
print("H‚ÇÅ: Los datos no siguen una distribuci√≥n normal.")

# Grupo 1
stat_s1, p_s1 = stats.shapiro(grupo1)
print(f"\nGrupo 1: Estad√≠stico W = {stat_s1:.3f}, p-valor = {p_s1:.3f}")
if p_s1 > alpha:
    print(f"  Decisi√≥n: No se rechaza H‚ÇÄ. Se asume normalidad (p > {alpha}).")
else:
    print(f"  Decisi√≥n: Se rechaza H‚ÇÄ. No se asume normalidad (p <= {alpha}).")

# Grupo 2
stat_s2, p_s2 = stats.shapiro(grupo2)
print(f"\nGrupo 2: Estad√≠stico W = {stat_s2:.3f}, p-valor = {p_s2:.3f}")
if p_s2 > alpha:
    print(f"  Decisi√≥n: No se rechaza H‚ÇÄ. Se asume normalidad (p > {alpha}).")
else:
    print(f"  Decisi√≥n: Se rechaza H‚ÇÄ. No se asume normalidad (p <= {alpha}).")

# --- 4. Aplicar pruebas param√©trica y no param√©trica ---

# Primero, comprobamos la homogeneidad de varianzas para la prueba t
print("\n--- C. PRUEBA DE HOMOGENEIDAD DE VARIANZAS (Levene) ---")
print("H‚ÇÄ: Las varianzas de los grupos son iguales.")
print("H‚ÇÅ: Las varianzas de los grupos son diferentes.")
stat_l, p_l = stats.levene(grupo1, grupo2)
print(f"\nResultado: Estad√≠stico F = {stat_l:.2f}, p-valor = {p_l}")
if p_l > alpha:
    print(f"  Decisi√≥n: No se rechaza H‚ÇÄ. Se asume que las varianzas son iguales (p > {alpha}).")
    equal_variances = True
else:
    print(f"  Decisi√≥n: Se rechaza H‚ÇÄ. Las varianzas son diferentes (p <= {alpha}).")
    equal_variances = False

print("\n--- D. APLICACI√ìN DE PRUEBAS DE HIP√ìTESIS ---")
print("Pregunta: ¬øExiste una diferencia significativa entre los dos grupos?")
print("H‚ÇÄ: Œº‚ÇÅ = Œº‚ÇÇ (No hay diferencia entre las medias de los grupos)")
print("H‚ÇÅ: Œº‚ÇÅ ‚â† Œº‚ÇÇ (Existe una diferencia entre las medias de los grupos)")

# Prueba Param√©trica: t de Student para muestras independientes
print("\n1. Prueba Param√©trica: t de Student para Muestras Independientes")
if not equal_variances:
    print("(Se aplicar√° la correcci√≥n de Welch porque las varianzas no son iguales)")

stat_t, p_t = stats.ttest_ind(grupo1, grupo2, equal_var=equal_variances)
print(f"  Estad√≠stico t = {stat_t:.3f}")
print(f"  p-valor = {p_t}")
if p_t < alpha:
    print(f"  Conclusi√≥n: Se rechaza H‚ÇÄ. Existe una diferencia estad√≠sticamente significativa entre los grupos (p < {alpha}).")
else:
    print(f"  Conclusi√≥n: No se rechaza H‚ÇÄ. No hay evidencia de una diferencia significativa (p >= {alpha}).")


# Prueba No Param√©trica: U de Mann-Whitney
print("\n2. Prueba No Param√©trica: U de Mann-Whitney")
stat_u, p_u = stats.mannwhitneyu(grupo1, grupo2, alternative='two-sided')
print(f"  Estad√≠stico U = {stat_u}")
print(f"  p-valor = {p_u}")
if p_u < alpha:
    print(f"  Conclusi√≥n: Se rechaza H‚ÇÄ. Existe una diferencia estad√≠sticamente significativa entre las distribuciones de los grupos (p < {alpha}).")
else:
    print(f"  Conclusi√≥n: No se rechaza H‚ÇÄ. No hay evidencia de una diferencia significativa (p >= {alpha}).")

print("\n" + "="*50)
print("CONCLUSI√ìN FINAL DEL EJERCICIO")
print("="*50)
print("Ambas pruebas, la param√©trica (t de Welch) y la no param√©trica (U de Mann-Whitney),")
print(f"arrojaron p-valores extremadamente bajos (p < {alpha}), lo que proporciona evidencia")
print("contundente para rechazar la hip√≥tesis nula. Por lo tanto, se concluye que")
print("existe una diferencia muy significativa entre el Grupo 1 y el Grupo 2.")
```
Con lo que tendremos el siguiente analisis estadistico

==================================================
### AN√ÅLISIS ESTAD√çSTICO PARA DOS GRUPOS INDEPENDIENTES
==================================================

--- A. AN√ÅLISIS DESCRIPTIVO ---
Grupo 1 (n=54):
  - Media: 62.138
  - Mediana: 64.523
  - Asimetr√≠a (Skewness): 0.116

Grupo 2 (n=51):
  - Media: 0.767
  - Mediana: 0.776
  - Asimetr√≠a (Skewness): -0.215

--- B. PRUEBAS DE NORMALIDAD (Shapiro-Wilk) --- <br/>
H‚ÇÄ: Los datos siguen una distribuci√≥n normal.<br/>
H‚ÇÅ: Los datos no siguen una distribuci√≥n normal.

Grupo 1: Estad√≠stico W = 0.969, p-valor = 0.171
  - Decisi√≥n: No se rechaza H‚ÇÄ. Se asume normalidad (p > 0.05).

Grupo 2: Estad√≠stico W = 0.987, p-valor = 0.860
  - Decisi√≥n: No se rechaza H‚ÇÄ. Se asume normalidad (p > 0.05).
...
Ambas pruebas, la param√©trica (t de Welch) y la no param√©trica (U de Mann-Whitney),
arrojaron p-valores extremadamente bajos (p < 0.05), lo que proporciona evidencia
contundente para rechazar la hip√≥tesis nula. Por lo tanto, se concluye que
existe una diferencia muy significativa entre el Grupo 1 y el Grupo 2. consistente bajo m√©todos param√©tricos y no param√©tricos.

---
El punto D indica lo siguiente:
D. (15%) Con los siguientes datos, realizar un ANOVA factorial:

- Realizar pruebas de normalidad (plantear la hip√≥tesis de normalidad).
- (Contestar a la pregunta realizada con base en los resultados de laspruebas).

Ejecucion en R
```R
# Datos originales
medida <- c(2.1, 2.2, 1.8, 2, 1.9, 2.2, 2.6, 2.7, 2.5, 2.8, 
            1.8, 1.9, 1.6, 2, 1.9, 2.1, 2, 2.2, 2.4, 2.1)

factorA <- gl(4, 5)
factorB <- factor(rep(1:5, 4))

# Crear data frame
datos <- data.frame(medida, factorA, factorB)

# Modelo ANOVA sin interacci√≥n (efectos principales solamente)
modelo <- aov(medida ~ factorA + factorB, data = datos)

# Verificar residuos
residuos <- resid(modelo)
print("Primeros residuos:")
print(head(residuos))

# Prueba de normalidad (ahora funcionar√°)
shapiro_resultado <- shapiro.test(residuos)
print(shapiro_resultado)

# ANOVA factorial
print("ANOVA - Efectos principales:")
summary(modelo)

```
### Resultados

<p align="center">
  <img src="./Imagenes/Punto 2 D en R.png", title="Solucion Punto 2-D" width="600"/>
</p>

- El test de Shapiro‚ÄìWilk es una de las pruebas estad√≠sticas m√°s utilizadas para verificar si un conjunto de datos sigue una distribuci√≥n normal.

==============================
### AN√ÅLISIS ESTAD√çSTICO 
==============================
</p>
Para interpretar un ANOVA, es necesario comprobar que los residuos del modelo siguen una distribuci√≥n normal.

- Hip√≥tesis nula (H‚ÇÄ): los residuos provienen de una distribuci√≥n normal.

- Hip√≥tesis alternativa (H‚ÇÅ): los residuos no provienen de una distribuci√≥n normal.

Dado que el test de Shapiro-Wilk aplicado a los residuos del modelo arroj√≥ un estad√≠stico W = 0.943 y un p-valor = 0.275.
Dado que p > 0.05, no se rechaza la hip√≥tesis nula, lo cual indica que el supuesto de normalidad se cumple.

por lo que:
- El factor A presenta un efecto altamente significativo sobre la variable medida (p < 0.001), osea influye significativamente en la variable respuesta.

- El factor B no muestra un efecto estad√≠sticamente significativo (p ‚âà 0.64), sobre la variable respuesta.



---

# An√°lisis-de-punto-3

<p align="center">
  <img src="./Imagenes/Punto 3.png", title="Pregunta Punto 3" width="600"/>
</p>


## Interpretaci√≥n_t√©cnica_de_la_matriz

- Estructura:
  - Filas = etiqueta real (True label).
  - Columnas = etiqueta predicha (Predicted label).
  - Cada celda (ùëñ,ùëó) (i= eje x,j= eje y) es el n√∫mero de ejemplos de la clase real i que el modelo predijo como j.
  - La diagonal contiene los aciertos (TP de cada clase). Las celdas fuera de diagonal son errores: a la izquierda/derecha se ven los patrones de confusi√≥n entre clases
- Patrones destacados en la matriz dada
  - La clase 2:tiene un desempe√±o perfecto (444 aciertos, sin confusiones), y una Precisi√≥n, recall y F1 = 1.00.

Ejecucion en python de la replica de la Matriz dada.
```python
import numpy as np
import warnings
# import gc
import seaborn as sns
import matplotlib.pyplot as plt
# import unicodedata
import numpy as np
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    f1_score,
    cohen_kappa_score,
    matthews_corrcoef
)

from sklearn.metrics import precision_recall_fscore_support
warnings.filterwarnings('ignore')

# Matriz de confusion
cm = np.array([
    [12, 26,  0,  4,  7, 21,  4, 1],  # clase real 0
    [22, 125, 0, 14, 43, 85, 12, 4],  # clase real 1
    [ 0,  0, 444, 0,  0,  0,  0, 0],  # clase real 2
    [ 4,  7,  0,  6,  1,  3,  0, 2],  # clase real 3
    [ 7, 34,  0,  2, 59, 14,  6, 3],  # clase real 4
    [13, 78,  0,  9, 26, 80,  7, 7],  # clase real 5
    [ 0, 11,  0,  4, 13, 13,  1, 1],  # clase real 6
    [ 1,  9,  0,  1,  3,  5,  1, 1]   # clase real 7
])

# Convertir la matriz de confusi√≥n en vectores de etiquetas verdaderas y predichas
y_true = []
y_pred = []
n_classes = cm.shape[0]
for true_label in range(n_classes):
    for pred_label in range(n_classes):
        count = cm[true_label, pred_label]
        # A√±adimos tantas repeticiones como indique la celda (i,j)
        y_true += [true_label] * count
        y_pred += [pred_label] * count
y_true = np.array(y_true)
y_pred = np.array(y_pred)

# Calcular precisi√≥n, recall, F1-score y soporte por clase
precision, recall, f1, support = precision_recall_fscore_support(
    y_true, y_pred, labels=list(range(n_classes))
)

# Mostrar resultados por clase
for cls in range(n_classes):
    print(
        f"Clase {cls}: "
        f"P={precision[cls]:.3f}, "
        f"R={recall[cls]:.3f}, "
        f"F1={f1[cls]:.3f} "
        f"(n={support[cls]})"
    )


## Variables Globales
# 1) Exactitud micro (accuracy)
micro_accuracy = accuracy_score(y_true, y_pred)

# 2) Balanced accuracy (macro-recall)
balanced_acc = balanced_accuracy_score(y_true, y_pred)

# 3) Macro-F1 y Weighted-F1
macro_f1 = f1_score(y_true, y_pred, average="macro")
weighted_f1 = f1_score(y_true, y_pred, average="weighted")

# 4) Cohen‚Äôs kappa
kappa = cohen_kappa_score(y_true, y_pred)

# 5) MCC multiclass
mcc = matthews_corrcoef(y_true, y_pred)

# Mostrar resultados
print(f"Exactitud (micro): {micro_accuracy:.3f}")
print(f"Balanced accuracy (macro-recall): {balanced_acc:.3f}")
print(f"Macro-F1: {macro_f1:.3f}")
print(f"Weighted-F1: {weighted_f1:.3f}")
print(f"Cohen‚Äôs kappa: {kappa:.3f}")
print(f"MCC multiclass: {mcc:.3f}")

```
Salida del modelo:
- Clase 0: P=0.203, R=0.160, F1=0.179 (n=75)
- Clase 1: P=0.431, R=0.410, F1=0.420 (n=305)
- Clase 2: P=1.000, R=1.000, F1=1.000 (n=444)
- Clase 3: P=0.150, R=0.261, F1=0.190 (n=23)
- Clase 4: P=0.388, R=0.472, F1=0.426 (n=125)
- Clase 5: P=0.362, R=0.364, F1=0.363 (n=220)
- Clase 6: P=0.032, R=0.023, F1=0.027 (n=43)
- Clase 7: P=0.053, R=0.048, F1=0.050 (n=21)

Metricas Globales
- Exactitud (micro): 0.580
- Balanced accuracy (macro-recall): 0.342
- Macro-F1: 0.332
- Weighted-F1: 0.577
- Cohen‚Äôs kappa: 0.455
- MCC multiclass: 0.456


Clases 1 y 5: fuerte confusi√≥n cruzada.

- Muchos verdaderos 1 se predicen como 5 (85).

- Muchos verdaderos 5 se predicen como 1 (78). El modelo mezcla 1 ‚Üî 5, sugiere fronteras poco separables o desbalance

Clases 6 y 7: muy baja sensibilidad (apenas 1 acierto en cada caso).
- Recall 6 ‚âà 0.023, Recall 7 ‚âà 0.048; probablemente poca evidencia o variables poco informativas para esas clases.

Las clases dominantes:
- la clase 2 domina con 444 aciertos , seguida por la clase 1 con 125 aciertos y la clase 5 con 80 aciertos.

Datos Generales: 
- El modelo es excelente para la clase 2, aceptable-regular para 1, 4 y 5, y muy d√©bil para 6 y 7. Existe confusi√≥n sistem√°tica 1‚Üî5. Conjunto desequilibrado, por lo que conviene ponderar por clase, remuestrear o ajustar umbrales.

## Metricas_para_dicha_Matriz
B. ¬øCu√°les y cu√°ntas m√©tricas puede obtener de dicha matriz?

De una matriz de confusi√≥n de N clases se obtienen, para cada clase n

### Conteos b√°sicos

- **TP‚Ççc‚Çé**: Verdaderos positivos (celda de la diagonal).  
- **FP‚Ççc‚Çé**: Falsos positivos (columna *c* fuera de la diagonal).  
- **FN‚Ççc‚Çé**: Falsos negativos (fila *c* fuera de la diagonal).  
- **TN‚Ççc‚Çé**: Verdaderos negativos.  
  \[
  TN_c = N - TP_c - FP_c - FN_c
  \]

- Tasas y medidas por clase
---

| M√©trica                                | F√≥rmula                                                                                                        | Interpretaci√≥n                                                                     |
| -------------------------------------- | -------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| **Precisi√≥n (PPV)**                    | $\mathrm{Precision}_c = \frac{\mathrm{TP}_c}{\mathrm{TP}_c + \mathrm{FP}_c}$                                   | Proporci√≥n de predicciones de $c$ que son correctas.                               |
| **Sensibilidad / Recall**              | $\mathrm{Recall}_c = \frac{\mathrm{TP}_c}{\mathrm{TP}_c + \mathrm{FN}_c}$                                      | Proporci√≥n de ejemplos de $c$ detectados correctamente.                            |
| **Especificidad (TNR)**                | $\mathrm{TNR}_c = \frac{\mathrm{TN}_c}{\mathrm{TN}_c + \mathrm{FP}_c}$                                         | Capacidad para evitar falsos positivos de $c$.                                     |
| **Tasa de falsos positivos (FPR)**     | $\mathrm{FPR}_c = 1 - \mathrm{TNR}_c = \frac{\mathrm{FP}_c}{\mathrm{FP}_c + \mathrm{TN}_c}$                    | Frecuencia con que se confunden otras clases con $c$.                              |
| **Tasa de falsos negativos (FNR)**     | $\mathrm{FNR}_c = 1 - \mathrm{Recall}_c = \frac{\mathrm{FN}_c}{\mathrm{FN}_c + \mathrm{TP}_c}$                 | Proporci√≥n de ejemplos de $c$ que se pierden.                                      |
| **Valor predictivo negativo (NPV)**    | $\mathrm{NPV}_c = \frac{\mathrm{TN}_c}{\mathrm{TN}_c + \mathrm{FN}_c}$                                         | Probabilidad de que un ejemplo no etiquetado como $c$ sea realmente de otra clase. |
| **Tasa de descubrimiento falsa (FDR)** | $\mathrm{FDR}_c = 1 - \mathrm{Precision}_c$                                                                    | Proporci√≥n de predicciones de $c$ que son err√≥neas.                                |
| **Tasa de falta de predicci√≥n (FOR)**  | $\mathrm{FOR}_c = 1 - \mathrm{NPV}_c$                                                                          | Proporci√≥n de negativos predichos para $c$ que en realidad son $c$.                |
| **F1-score**                           | $F1_c = \frac{2 \cdot \mathrm{Precision}_c \cdot \mathrm{Recall}_c}{\mathrm{Precision}_c + \mathrm{Recall}_c}$ | Media arm√≥nica entre precisi√≥n y recall.                                           |
| **Balanced accuracy (por clase)**      | $\mathrm{BA}_c = \tfrac{1}{2}(\mathrm{Recall}_c + \mathrm{TNR}_c)$                                             | Valor medio entre sensibilidad y especificidad.                                    |
| **Prevalencia**                        | $\mathrm{Prev}_c = \frac{\mathrm{TP}_c + \mathrm{FN}_c}{N}$                                                    | Fracci√≥n de ejemplos verdaderos de la clase $c$.                                   |
| **Prevalencia predicha**               | $\mathrm{PrevPred}_c = \frac{\mathrm{TP}_c + \mathrm{FP}_c}{N}$                                                | Fracci√≥n de ejemplos que el modelo predice como $c$.                               |

---

### Agregaciones multiclass

- **Macro**: promedio no ponderado (Precision/Recall/F1 macro).  
- **Weighted**: promedio ponderado por soporte (Precision/Recall/F1 weighted).  
- **Micro**: cuenta global de TP, FP, FN (Precision/Recall/F1 micro = exactitud global).  
- **Exactitud global**, **Balanced accuracy macro**, **Cohen‚Äôs Œ∫**, **MCC multiclass**.

---

### ¬øCu√°ntas m√©tricas?

Con \\(C = 8\\):

- **32 conteos** (TP/FP/FN/TN √ó 8). donde para cada clase en un problema multiclase se pueden defini 4¬†conteos¬†basicos¬†por¬†clase√ó8¬†clases=32¬†conteos¬†en¬†total
- **‚âà 10‚Äì12 tasas por clase** (ej. Precision, Recall, F1, TNR, FPR, FNR, NPV, FDR, FOR, BA‚Ä¶) ‚Üí **80‚Äì96 m√©tricas**.  que salen de 10 metricas x 8 clases =80 metricas, 12 metricas x 8 clases =96 metricas
- **‚â• 10 m√©tricas globales** (macro/micro/weighted de P/R/F1, accuracy, balanced accuracy, Œ∫, MCC, etc.).  





	Fracci√≥n de ejemplos que el modelo predice co
---
